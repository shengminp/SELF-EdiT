{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41cddaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from senteval import utils\n",
    "from simcse.models import Pooler, MLPLayer, Similarity\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ef9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = \"qed\"\n",
    "PRO_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "TOKENIZER_PATH = os.path.join(PRO_PATH, 'fairseq_mo', 'utils')\n",
    "if TOKENIZER_PATH not in sys.path:\n",
    "    sys.path = [TOKENIZER_PATH] + sys.path\n",
    "from tokenizer import selfies_tokenizer, atomwise_tokenizer, MoTokenizer\n",
    "CHECKPOINT_PATH = os.path.join(PRO_PATH, 'checkpoints', DATA_TYPE, 'simcse')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72905a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentemb_forward(\n",
    "    cls,\n",
    "    encoder,\n",
    "    input_ids=None,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    labels=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "):\n",
    "\n",
    "    return_dict = return_dict if return_dict is not None else cls.config.use_return_dict\n",
    "\n",
    "    outputs = encoder(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=True if cls.pooler_type in ['avg_top2', 'avg_first_last'] else False,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    pooler_output = cls.pooler(attention_mask, outputs)\n",
    "    if cls.pooler_type == \"cls\":\n",
    "        pooler_output = cls.mlp(pooler_output)\n",
    "\n",
    "    if not return_dict:\n",
    "        return (outputs[0], pooler_output) + outputs[2:]\n",
    "\n",
    "    return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "        pooler_output=pooler_output,\n",
    "        last_hidden_state=outputs.last_hidden_state,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90ef7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForExtract(BertPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "    \n",
    "    def __init__(self, config, *model_args, **model_kargs):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "\n",
    "        self.pooler_type = \"cls\"\n",
    "        self.pooler = Pooler(\"cls\")\n",
    "        self.mlp = MLPLayer(config)\n",
    "        self.sim = Similarity(temp=0.05)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        sent_emb=False,\n",
    "        mlm_input_ids=None,\n",
    "        mlm_labels=None,\n",
    "    ):\n",
    "        if sent_emb:\n",
    "            return sentemb_forward(self, self.bert,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        else:\n",
    "            return cl_forward(self, self.bert,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=labels,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                mlm_input_ids=mlm_input_ids,\n",
    "                mlm_labels=mlm_labels,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e59511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForExtract.from_pretrained(CHECKPOINT_PATH)\n",
    "model = model.to(DEVICE)\n",
    "tokenizer = MoTokenizer.from_pretrained(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f6f551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'usepytorch': True, 'kfold': 10}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 64,\n",
    "                                 'tenacity': 5, 'epoch_size': 4}\n",
    "params = utils.dotdict(params)\n",
    "params.usepytorch = True if 'usepytorch' not in params else params.usepytorch\n",
    "params.seed = 1111 if 'seed' not in params else params.seed\n",
    "\n",
    "params.batch_size = 128 if 'batch_size' not in params else params.batch_size\n",
    "params.nhid = 0 if 'nhid' not in params else params.nhid\n",
    "params.kfold = 5 if 'kfold' not in params else params.kfold\n",
    "\n",
    "if 'classifier' not in params or not params['classifier']:\n",
    "    params.classifier = {'nhid': 0}\n",
    "\n",
    "assert 'nhid' in params.classifier, 'Set number of hidden units in classifier config!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b46cfda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), DATA_TYPE, 'bin_data', 'dict.low.txt'), 'r') as fp:\n",
    "    data = [line.strip() for line in fp.readlines()]\n",
    "vocab = [item.split()[0] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82d6ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(batch, max_length=None):\n",
    "    # Tokenization\n",
    "    if max_length is not None:\n",
    "        batch = tokenizer.batch_encode_plus(\n",
    "            batch,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "    else:\n",
    "        batch = tokenizer.batch_encode_plus(\n",
    "            batch,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "        )\n",
    "\n",
    "    # Move to the correct device\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(DEVICE)\n",
    "\n",
    "    # Get raw embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch, output_hidden_states=True, return_dict=True, sent_emb=True)\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        pooler_output = outputs.pooler_output\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "    return pooler_output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57502020",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings_dict = dict()\n",
    "for i in range(0, len(vocab), params.batch_size):\n",
    "    batch = vocab[i:i+params.batch_size]\n",
    "    embeddings = batcher(batch, max_length=50)\n",
    "    for item in zip(batch, embeddings):\n",
    "        embeddings_dict[item[0]] = item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d59731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = [f\"{len(embeddings_dict)} {len(embeddings_dict[vocab[0]])}\"]\n",
    "for item in embeddings_dict.items():\n",
    "    emb = ' '.join([str(data) for data in item[1].tolist()])\n",
    "    data_line = item[0] + ' ' + emb\n",
    "    file_content.append(data_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c7f90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), DATA_TYPE, 'emb_data', 'dict.emb'), 'w') as fp:\n",
    "    fp.write('\\n'.join(file_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09ea94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
